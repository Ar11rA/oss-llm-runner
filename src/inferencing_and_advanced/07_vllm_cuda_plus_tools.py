# -*- coding: utf-8 -*-
"""peftloratrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x6w6GEbksjHAJMvR66FMhi_9usCPIqoF

## Nvidia Check
"""

# !nvidia-smi

"""## vLLM Check"""

# !pip uninstall vllm -y
# !pip install vllm==0.11.2

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["VLLM_USE_V1"] = "0"
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

from vllm import LLM, SamplingParams

import gc
import torch

# Now recreate
llm = LLM(
    model="Qwen/Qwen3-0.6B",
    enforce_eager=True,
    gpu_memory_utilization=0.7,
)
params = SamplingParams(temperature=0.7, max_tokens=2000)
outputs = llm.generate(["What are the symptoms of diabetes?"], params)
print(outputs[0].outputs[0].text)

"""## HF LoRA"""

# !pip install torch datasets transformers peft trl accelerate -q

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTConfig, SFTTrainer

print("Loading dataset...")
dataset = load_dataset("OpenMed/Medical-Reasoning-SFT-GPT-OSS-120B")

print(f"Dataset size: {len(dataset['train'])} samples")

def format_conversation(example):
    """Convert messages to a single text string."""
    conversation = ""
    for msg in example["messages"]:
        role = msg["role"]
        content = msg["content"]
        conversation += f"<|{role}|>\n{content}\n"
    conversation += "<|end|>"
    return {"text": conversation}


formatted_dataset = dataset.map(format_conversation)
print(f"Formatted dataset sample:\n{formatted_dataset['train'][0]['text'][:300]}...")

model_id = "Qwen/Qwen3-0.6B"  # 0.6B parameter model

# Detect device
if torch.backends.mps.is_available():
    device = "mps"
    dtype = torch.float32  # MPS doesn't support fp16 training
elif torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32

print(f"Loading model: {model_id}")
print(f"Using device: {device}, dtype: {dtype}")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# Set padding token (required for training)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=dtype,
    device_map="auto" if device != "mps" else None,
    trust_remote_code=True,
)

if device == "mps":
    model = model.to(device)

# Enable gradient checkpointing for memory efficiency
model.gradient_checkpointing_enable()

lora_config = LoraConfig(
    r=16,                          # Rank: balance between quality and efficiency
    lora_alpha=32,                 # Scaling factor (alpha/r = scaling)
    target_modules=[               # Qwen3 attention layers
        "q_proj",                  # Query projection
        "k_proj",                  # Key projection
        "v_proj",                  # Value projection
        "o_proj",                  # Output projection
        "gate_proj",               # FFN gate
        "up_proj",                 # FFN up projection
        "down_proj",               # FFN down projection
    ],
    lora_dropout=0.05,             # Small dropout for regularization
    bias="none",                   # Don't train biases
    task_type=TaskType.CAUSAL_LM,  # Causal language modeling
)

# Apply LoRA to model
model = get_peft_model(model, lora_config)

# Print trainable parameters
model.print_trainable_parameters()

training_args = SFTConfig(
    output_dir="./output_models/qwen3-0.6b-medical-lora",

    # Batch size settings
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,    # Effective batch = 2 * 16 = 32

    # Training duration
    num_train_epochs=1,
    max_steps=100,                     # Limit steps for testing

    # Learning rate (higher for LoRA)
    learning_rate=2e-4,
    warmup_ratio=0.03,

    # Optimizer
    optim="adamw_torch",
    weight_decay=0.01,

    # Precision
    fp16=(device == "cuda"),

    # Logging & Saving
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,

    # Memory optimization
    gradient_checkpointing=True,
    dataloader_pin_memory=False,       # Required for MPS

    # SFT-specific settings
    max_length=512,                    # Max sequence length
    packing=False,                     # Don't pack sequences

    # Misc
    report_to="none",
    remove_unused_columns=False,
)


# ============================================================================
# 6. Initialize SFTTrainer
# ============================================================================
def formatting_func(example):
    """Format each example for training."""
    return example["text"]

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=formatted_dataset["train"],
    processing_class=tokenizer,         # Use processing_class, not tokenizer
    formatting_func=formatting_func,
)

print("\n" + "="*60)
print("Starting LoRA Fine-tuning")
print("="*60)

trainer.train()


# ============================================================================
# 8. Save the LoRA Adapter
# ============================================================================
print("\nSaving LoRA adapter...")
model.save_pretrained("./output_models/qwen3-0.6b-medical-lora/final")
tokenizer.save_pretrained("./output_models/qwen3-0.6b-medical-lora/final")

print("Training complete!")

# FIRST CELL - Run before ANY other imports

del llm
gc.collect()
torch.cuda.empty_cache()

from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# Load base model with LoRA enabled
llm = LLM(
    model="Qwen/Qwen3-0.6B",
    enable_lora=True,
    max_lora_rank=16,  # Match your training config (r=16)
    enforce_eager=True,
    gpu_memory_utilization=0.7,
)

# Create LoRA request
lora_request = LoRARequest(
    "medical",  # adapter name
    1,  # adapter id
    "./output_models/qwen3-0.6b-medical-lora/final"  # path to PEFT adapter
)

params = SamplingParams(temperature=0.7, max_tokens=200)

# Generate with adapter
outputs = llm.generate(
    ["What are the symptoms of diabetes?"],
    params,
    lora_request=lora_request
)
print(outputs[0].outputs[0].text)

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"},
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
                },
                "required": ["location"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Calculate a math expression",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {"type": "string"}
                },
                "required": ["expression"]
            }
        }
    }
]

# Tool implementations
def get_weather(location, unit="celsius"):
    return {"location": location, "temp": 22, "unit": unit, "condition": "sunny"}

def calculate(expression):
    return {"result": eval(expression)}

TOOL_MAP = {"get_weather": get_weather, "calculate": calculate}

import json

def create_prompt(user_msg, tools):
    tools_json = json.dumps(tools, indent=2)
    return f"""<|im_start|>system
You have access to these tools:
{tools_json}

To use a tool, respond with JSON: {{"name": "tool_name", "arguments": {{"arg": "value"}}}}
<|im_end|>
<|im_start|>user
{user_msg}
<|im_end|>
<|im_start|>assistant
"""

# Parse tool call from response
def parse_tool_call(response):
    try:
        start = response.find("{")
        end = response.rfind("}") + 1
        if start != -1 and end > start:
            data = json.loads(response[start:end])
            if "name" in data and "arguments" in data:
                return data
    except:
        pass
    return None

queries = [
    "What's the weather in Paris?",
    "Calculate 25 * 4 + 10",
    "What is cure for AIDS HIV?"
]

params = SamplingParams(temperature=0.7, max_tokens=200)

for query in queries:
    print(f"\n{'='*50}")
    print(f"USER: {query}")

    prompt = create_prompt(query, tools)
    output = llm.generate([prompt], params)[0]
    response = output.outputs[0].text

    print(f"MODEL: {response}")

    tool_call = parse_tool_call(response)
    if tool_call:
        result = TOOL_MAP[tool_call["name"]](**tool_call["arguments"])
        print(f"ðŸ”§ TOOL: {tool_call['name']}({tool_call['arguments']})")
        print(f"ðŸ“¤ RESULT: {result}")
