# LLM Deeper: Project Overview

This document provides an overview of the LLM Deeper project, introduces core LLM concepts, and guides you through the learning path.

---

## Table of Contents

1. [What is a Large Language Model?](#what-is-a-large-language-model)
2. [The Transformer Architecture](#the-transformer-architecture)
3. [Pre-training vs Fine-tuning vs Inference](#pre-training-vs-fine-tuning-vs-inference)
4. [Parameter Counts and Model Sizes](#parameter-counts-and-model-sizes)
5. [Learning Path](#learning-path)
6. [File Index](#file-index)
7. [Hardware Requirements](#hardware-requirements)

---

## What is a Large Language Model?

A **Large Language Model (LLM)** is a neural network trained to predict the next token (word piece) given previous tokens. Despite this simple objective, LLMs develop remarkable capabilities:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM: Next Token Prediction                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Input:  "The capital of France is"                                        â”‚
â”‚                     â†“                                                       â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚           â”‚   LLM Model     â”‚                                               â”‚
â”‚           â”‚                 â”‚                                               â”‚
â”‚           â”‚  P("Paris") = 0.92                                              â”‚
â”‚           â”‚  P("Lyon")  = 0.03                                              â”‚
â”‚           â”‚  P("the")   = 0.01                                              â”‚
â”‚           â”‚  ...                                                            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                     â†“                                                       â”‚
â”‚   Output: "Paris"                                                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Properties

| Property | Description |
|----------|-------------|
| **Autoregressive** | Generates one token at a time, feeding output back as input |
| **Context Window** | Can "see" a fixed number of previous tokens (e.g., 4K, 32K, 128K) |
| **Emergent Abilities** | Larger models exhibit capabilities not present in smaller ones |
| **In-Context Learning** | Can learn from examples in the prompt without weight updates |

### Why "Large"?

| Model | Parameters | Training Data | Training Compute |
|-------|------------|---------------|------------------|
| GPT-2 (2019) | 1.5B | 40GB text | ~$50K |
| GPT-3 (2020) | 175B | 570GB text | ~$5M |
| GPT-4 (2023) | ~1.8T* | Unknown | ~$100M+ |
| Llama 3.1 (2024) | 405B | 15T tokens | Unknown |

*Estimated, not officially confirmed

---

## The Transformer Architecture

The **Transformer** (Vaswani et al., 2017) is the architecture behind all modern LLMs. It consists of stacked blocks, each containing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Transformer Block                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Input: (batch, seq_len, d_model)                                          â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚        Multi-Head Self-Attention           â”‚                           â”‚
â”‚   â”‚                                            â”‚                           â”‚
â”‚   â”‚  â€¢ Split into multiple "heads"             â”‚                           â”‚
â”‚   â”‚  â€¢ Each head: Attention(Q, K, V)           â”‚                           â”‚
â”‚   â”‚  â€¢ Concatenate and project                 â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼  + Residual Connection                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚           Layer Normalization              â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚        Feed-Forward Network (FFN)          â”‚                           â”‚
â”‚   â”‚                                            â”‚                           â”‚
â”‚   â”‚  â€¢ Dense(d_model â†’ 4Ã—d_model, activation)  â”‚                           â”‚
â”‚   â”‚  â€¢ Dense(4Ã—d_model â†’ d_model)              â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼  + Residual Connection                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚           Layer Normalization              â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚   Output: (batch, seq_len, d_model)                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Attention Formula

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

**Intuition:**
- **Q (Query)**: "What am I looking for?"
- **K (Key)**: "What do I contain?"
- **V (Value)**: "What information do I provide?"
- **QK^T**: Compute similarity between all token pairs
- **softmax**: Convert to probabilities (attention weights)
- **âˆšd_k**: Scaling to prevent gradient saturation

### Causal Masking

For language models, we mask future tokens so position `i` can only attend to positions `0...i`:

```
         Position 0  1  2  3
Position 0:  [1.0, -âˆ, -âˆ, -âˆ]   â† Can only see itself
Position 1:  [0.5, 0.5, -âˆ, -âˆ]  â† Can see 0 and itself
Position 2:  [0.3, 0.3, 0.4, -âˆ] â† Can see 0, 1, and itself
Position 3:  [0.2, 0.3, 0.2, 0.3] â† Can see everything
```

---

## Pre-training vs Fine-tuning vs Inference

### The Three Phases of LLM Usage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLM Development Lifecycle                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  PRE-TRAINING   â”‚â”€â”€â”€â–¶â”‚  FINE-TUNING    â”‚â”€â”€â”€â–¶â”‚   INFERENCE     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Internet-scale       â€¢ Task-specific         â€¢ Serve predictions        â”‚
â”‚    text data              dataset               â€¢ Real-time                â”‚
â”‚  â€¢ Next-token           â€¢ Instruction           â€¢ Optimized for            â”‚
â”‚    prediction             following               latency                   â”‚
â”‚  â€¢ Months of GPU        â€¢ Hours to days         â€¢ Milliseconds per         â”‚
â”‚    time                 â€¢ Consumer GPU            token                     â”‚
â”‚  â€¢ $1M - $100M+         â€¢ $10 - $1000           â€¢ $0.001 per request       â”‚
â”‚                                                                             â”‚
â”‚  YOU: Never do this     YOU: Do this!           YOU: Deploy this!          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison Table

| Phase | Data | Compute | Cost | Result |
|-------|------|---------|------|--------|
| **Pre-training** | Trillions of tokens | 1000s of GPUs for months | $1M-$100M | Foundation model |
| **Fine-tuning** | 1K-1M examples | 1 GPU for hours | $10-$1000 | Specialized model |
| **Inference** | User queries | 1 GPU per request | $0.001/request | Predictions |

### Types of Fine-tuning

| Method | Trainable Params | Memory | Speed | Quality |
|--------|-----------------|--------|-------|---------|
| **Full Fine-tuning** | 100% | High | Slow | Best |
| **LoRA** | 0.1-1% | Low | Fast | Very Good |
| **QLoRA** | 0.1-1% | Very Low | Fast | Good |
| **Prompt Tuning** | <0.01% | Minimal | Fastest | Moderate |

---

## Parameter Counts and Model Sizes

### Understanding Model Sizes

A parameter is a single number (weight) in the model. More parameters = more capacity = better performance (usually).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Parameter Count Formula                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   For a Transformer with:                                                   â”‚
â”‚   â€¢ L layers                                                                â”‚
â”‚   â€¢ d_model embedding dimension                                             â”‚
â”‚   â€¢ V vocabulary size                                                       â”‚
â”‚   â€¢ h attention heads                                                       â”‚
â”‚                                                                             â”‚
â”‚   Approximate parameters:                                                   â”‚
â”‚                                                                             â”‚
â”‚   Embeddings:     V Ã— d_model                                               â”‚
â”‚   Per Layer:      12 Ã— d_modelÂ²  (attention + FFN)                         â”‚
â”‚   LM Head:        d_model Ã— V                                               â”‚
â”‚                                                                             â”‚
â”‚   Total â‰ˆ 2Ã—VÃ—d_model + LÃ—12Ã—d_modelÂ²                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Size to VRAM

| Parameters | FP32 | FP16/BF16 | INT8 | INT4 |
|------------|------|-----------|------|------|
| 0.5B | 2 GB | 1 GB | 0.5 GB | 0.25 GB |
| 1.5B | 6 GB | 3 GB | 1.5 GB | 0.75 GB |
| 7B | 28 GB | 14 GB | 7 GB | 3.5 GB |
| 13B | 52 GB | 26 GB | 13 GB | 6.5 GB |
| 70B | 280 GB | 140 GB | 70 GB | 35 GB |

**Formula:** `VRAM = Parameters Ã— Bytes per Parameter`
- FP32: 4 bytes/param
- FP16/BF16: 2 bytes/param
- INT8: 1 byte/param
- INT4: 0.5 bytes/param

---

## Learning Path

### Suggested Order

```
Week 1: Foundations (src/basics/)
â”œâ”€â”€ 00_pandas.py          - Data manipulation refresher
â”œâ”€â”€ 01_numpy.py           - Tensor operations
â”œâ”€â”€ 02_math.py            - Mathematical foundations
â”œâ”€â”€ 03_attention_math.py  - Attention step-by-step â­
â”œâ”€â”€ 04_attention_impl.py  - Full attention implementation
â”œâ”€â”€ 05_attention_positional_encoding.py - Position info
â”œâ”€â”€ 06_mha.py             - Multi-head attention â­
â””â”€â”€ 07_conclusion.py      - Putting it together

Week 2: Training (src/hfcore/)
â”œâ”€â”€ 00_tinygpt.py         - Build GPT from scratch â­â­â­
â”œâ”€â”€ 01_transformers.py    - HuggingFace basics
â”œâ”€â”€ 02_peft_lora.py       - LoRA concepts
â”œâ”€â”€ 03_dataset_plus_finetune_full.py - Full fine-tuning
â”œâ”€â”€ 04_dataset_plus_finetune_lora.py - LoRA fine-tuning â­â­
â”œâ”€â”€ 05_finetuned_model_inference.py
â””â”€â”€ 06_evals.py           - Evaluation metrics

Week 3+: Production (src/inferencing_and_advanced/)
â”œâ”€â”€ 00_base.py
â”œâ”€â”€ 01_peft_to_mlx.py     - Convert adapters
â”œâ”€â”€ 02_mlx_lora.py        - MLX inference
â”œâ”€â”€ 03_server.py          - FastAPI server
â”œâ”€â”€ 04_tool_calling_basic.py - Function calling â­
â”œâ”€â”€ 05_tool_calling_advanced.py - Advanced patterns
â”œâ”€â”€ 07_vllm_cuda_plus_tools.py - Production vLLM â­â­
â”œâ”€â”€ 08_deepseek_example.py - DeepSeek R1 reasoning â­
â”œâ”€â”€ 09_mistral_example.py  - Large model with FP8 â­
â””â”€â”€ 10_diffuser_image.py   - Image generation ğŸ¨
```

â­ = Highly recommended

---

## File Index

### src/basics/

| File | Description | Key Concepts |
|------|-------------|--------------|
| `00_pandas.py` | Data manipulation basics | DataFrames, filtering |
| `01_numpy.py` | Tensor operations | Shapes, broadcasting, matmul |
| `02_math.py` | Mathematical foundations | Linear algebra basics |
| `03_attention_math.py` | Attention step-by-step | Q, K, V, scores, softmax |
| `04_attention_impl.py` | Full attention | Complete implementation |
| `05_attention_positional_encoding.py` | Position information | Sinusoidal, learned |
| `06_mha.py` | Multi-head attention | Head splitting, concatenation |
| `07_conclusion.py` | Summary | Putting it all together |

### src/hfcore/

| File | Description | Key Concepts |
|------|-------------|--------------|
| `00_tinygpt.py` | Minimal GPT from scratch | Tokenization, training loop |
| `01_transformers.py` | HuggingFace basics | AutoModel, AutoTokenizer |
| `02_peft_lora.py` | LoRA concepts | Low-rank adaptation |
| `03_dataset_plus_finetune_full.py` | Full fine-tuning | All parameters updated |
| `04_dataset_plus_finetune_lora.py` | LoRA fine-tuning | Efficient adaptation |
| `05_finetuned_model_inference.py` | Using fine-tuned models | Loading, generation |
| `06_evals.py` | Evaluation metrics | ROUGE, BERTScore, perplexity |

### src/inferencing_and_advanced/

| File | Description | Key Concepts |
|------|-------------|--------------|
| `00_base.py` | Base concepts | Setup |
| `01_peft_to_mlx.py` | Convert PEFT â†’ MLX | Adapter formats |
| `02_mlx_lora.py` | MLX inference | Apple Silicon |
| `03_server.py` | FastAPI server | REST API |
| `04_tool_calling_basic.py` | Function calling | Tool schemas |
| `05_tool_calling_advanced.py` | Advanced patterns | Multi-turn, registry |
| `07_vllm_cuda_plus_tools.py` | Production vLLM | CUDA, KV cache |
| `08_deepseek_example.py` | DeepSeek R1 reasoning | Thinking tokens, chat template |
| `09_mistral_example.py` | Mistral Devstral 24B | FP8 quantization, code generation |
| `10_diffuser_image.py` | Image generation | Stable Diffusion 3.5, diffusers |

---

## Hardware Requirements

### Minimum Requirements by Task

| Task | CPU | Apple Silicon | NVIDIA GPU |
|------|-----|---------------|------------|
| **Basics (attention math)** | âœ… Any | âœ… Any | Not needed |
| **TinyGPT training** | âœ… Any | âœ… Any | Not needed |
| **LoRA fine-tuning (0.5B)** | âš ï¸ Slow | âœ… M1+ 8GB | âœ… 8GB+ |
| **LoRA fine-tuning (7B)** | âŒ | âœ… M1+ 16GB | âœ… 16GB+ |
| **vLLM inference** | âŒ | âŒ | âœ… Required |
| **MLX inference** | âŒ | âœ… Required | âŒ |

### Recommended Setups

| Budget | Setup | What You Can Do |
|--------|-------|-----------------|
| **$0** | Google Colab (free) | Basic training, limited time |
| **$1K** | Mac Mini M2 16GB | Full LoRA fine-tuning, MLX inference |
| **$2K** | Mac Studio M2 32GB | 7B+ models, fast inference |
| **$1K/mo** | Cloud GPU (A100) | Production vLLM, 70B models |

---

## Next Steps

1. **Start with fundamentals**: [01_basics.md](01_basics.md)
2. **Learn training**: [02_hfcore.md](02_hfcore.md)
3. **Deploy models**: [03_inferencing.md](03_inferencing.md)
4. **Add capabilities**: [04_tool_calling.md](04_tool_calling.md)
5. **Go to production**: [05_deployment.md](05_deployment.md)

---

## Quick Reference

### Key Formulas

| Concept | Formula |
|---------|---------|
| Attention | `softmax(QK^T / âˆšd_k) Ã— V` |
| LoRA | `W' = W + BA` where `B: dÃ—r`, `A: rÃ—d` |
| Perplexity | `exp(loss)` |
| VRAM (weights) | `params Ã— bytes_per_param` |
| KV Cache | `2 Ã— layers Ã— hidden Ã— seq_len Ã— kv_heads Ã— 2` |

### Common Commands

```bash
# Run any script
uv run python src/path/to/file.py

# Start MLX server
uv run python -m mlx_lm.server --model MODEL --port 8000

# Start vLLM server
vllm serve MODEL --enable-lora --lora-modules name=path
```
