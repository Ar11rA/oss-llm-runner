# Model Inference

This document covers model inference: formats, quantization, MLX for Apple Silicon, and building inference servers.

---

## Table of Contents

1. [Model Formats](#model-formats)
2. [Quantization](#quantization)
3. [KV Cache Deep Dive](#kv-cache-deep-dive)
4. [MLX on Apple Silicon](#mlx-on-apple-silicon)
5. [Adapter Formats & Conversion](#adapter-formats--conversion)
6. [Building Inference Servers](#building-inference-servers)
7. [Code Walkthrough](#code-walkthrough)

---

## Model Formats

### Common Formats

| Format | Description | Used By |
|--------|-------------|---------|
| **PyTorch (.bin)** | Original PyTorch format | Legacy HuggingFace |
| **SafeTensors (.safetensors)** | Safe, fast loading | Modern HuggingFace |
| **GGUF** | Quantized for llama.cpp | Local inference |
| **MLX** | Apple Silicon optimized | MLX-LM |
| **ONNX** | Cross-platform | Production |
| **TensorRT** | NVIDIA optimized | High-performance |

### SafeTensors vs PyTorch

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SafeTensors Advantages                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   PyTorch .bin:                      SafeTensors:                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚ Uses pickle     â”‚                â”‚ No pickle       â”‚                   â”‚
â”‚   â”‚ (security risk) â”‚                â”‚ (safe by design)â”‚                   â”‚
â”‚   â”‚                 â”‚                â”‚                 â”‚                   â”‚
â”‚   â”‚ Full load       â”‚                â”‚ Lazy loading    â”‚                   â”‚
â”‚   â”‚ into memory     â”‚                â”‚ (memory-mapped) â”‚                   â”‚
â”‚   â”‚                 â”‚                â”‚                 â”‚                   â”‚
â”‚   â”‚ Slow loading    â”‚                â”‚ Fast loading    â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â”‚   Rule: Always prefer SafeTensors (.safetensors) when available            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quantization

### What is Quantization?

Quantization reduces the precision of model weights to save memory and speed up inference.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Quantization Levels                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   FP32:    32 bits per weight   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]         â”‚
â”‚            Full precision                                                   â”‚
â”‚                                                                             â”‚
â”‚   FP16:    16 bits per weight   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                         â”‚
â”‚            Half precision                                                   â”‚
â”‚                                                                             â”‚
â”‚   BF16:    16 bits per weight   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                         â”‚
â”‚            Brain floating point (better for training)                       â”‚
â”‚                                                                             â”‚
â”‚   INT8:    8 bits per weight    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                                 â”‚
â”‚            1/4 the memory of FP32                                          â”‚
â”‚                                                                             â”‚
â”‚   INT4:    4 bits per weight    [â–ˆâ–ˆâ–ˆâ–ˆ]                                     â”‚
â”‚            1/8 the memory of FP32                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Savings

| Precision | Bytes/Param | 7B Model | 70B Model |
|-----------|-------------|----------|-----------|
| FP32 | 4 | 28 GB | 280 GB |
| FP16/BF16 | 2 | 14 GB | 140 GB |
| INT8 | 1 | 7 GB | 70 GB |
| INT4 | 0.5 | 3.5 GB | 35 GB |

### Quality vs Compression Trade-offs

| Precision | Memory | Speed | Quality |
|-----------|--------|-------|---------|
| FP32 | 1Ã— | 1Ã— | Best |
| FP16 | 0.5Ã— | 1.5Ã— | Excellent |
| INT8 | 0.25Ã— | 2Ã— | Very Good |
| INT4 | 0.125Ã— | 2.5Ã— | Good |
| INT2 | 0.0625Ã— | 3Ã— | Degraded |

### When to Use Each

| Use Case | Precision |
|----------|-----------|
| Training | FP32 or BF16 |
| Fine-tuning | FP16 or BF16 |
| Production inference | FP16 or INT8 |
| Consumer hardware | INT4 (GGUF) |
| Extreme memory constraints | INT4 with GPTQ/AWQ |

---

## KV Cache Deep Dive

### What is KV Cache?

During autoregressive generation, the model computes Key and Value vectors for each token. The **KV Cache** stores these to avoid recomputation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Without KV Cache                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Token 1: Compute K,V for "The"                                           â”‚
â”‚   Token 2: Compute K,V for "The", "cat"         â† Redundant!               â”‚
â”‚   Token 3: Compute K,V for "The", "cat", "sat"  â† Very redundant!          â”‚
â”‚                                                                             â”‚
â”‚   Complexity: O(nÂ²) per generation step                                    â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                           With KV Cache                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Token 1: Compute K,V for "The"        â†’ Cache: [(Kâ‚,Vâ‚)]                 â”‚
â”‚   Token 2: Compute K,V for "cat" only   â†’ Cache: [(Kâ‚,Vâ‚), (Kâ‚‚,Vâ‚‚)]        â”‚
â”‚   Token 3: Compute K,V for "sat" only   â†’ Cache: [(Kâ‚,Vâ‚), (Kâ‚‚,Vâ‚‚), ...]   â”‚
â”‚                                                                             â”‚
â”‚   Complexity: O(n) per generation step                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### KV Cache Memory Formula

```
KV Cache Size = 2 Ã— num_layers Ã— 2 Ã— hidden_size Ã— num_kv_heads/num_heads Ã— max_seq_len Ã— batch_size Ã— bytes_per_param
```

Simplified:
```
KV Cache â‰ˆ 4 Ã— num_layers Ã— hidden_size Ã— kv_heads Ã— max_seq_len Ã— bytes
```

### KV Cache by Model Size

| Model | Layers | Hidden | KV Heads | 4K Context | 32K Context |
|-------|--------|--------|----------|------------|-------------|
| 0.5B | 28 | 1024 | 2 | 0.5 GB | 4 GB |
| 7B | 32 | 4096 | 8 | 2 GB | 16 GB |
| 13B | 40 | 5120 | 8 | 3 GB | 24 GB |
| 70B | 80 | 8192 | 8 | 10 GB | 80 GB |

### Why This Matters

**The model weights are often smaller than the KV cache!**

```
Example: Qwen3-0.6B with 40K context
â”œâ”€â”€ Model weights (FP16): 1.2 GB
â”œâ”€â”€ KV Cache: 9.2 GB
â””â”€â”€ Total VRAM: ~11 GB

Solution: Limit max_model_len!
```

```python
# Memory-efficient vLLM config
llm = LLM(
    model="Qwen/Qwen3-0.6B",
    max_model_len=2048,           # Limit context (vs 40,960 default)
    gpu_memory_utilization=0.7,   # Reserve 30% headroom
)
```

---

## MLX on Apple Silicon

### What is MLX?

MLX is Apple's machine learning framework, optimized for Apple Silicon (M1/M2/M3/M4).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MLX Architecture                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                    Unified Memory                           â”‚          â”‚
â”‚   â”‚                                                             â”‚          â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚          â”‚
â”‚   â”‚   â”‚   CPU       â”‚ â†â”€â”€â”€â”€â”€â†’ â”‚   GPU       â”‚                  â”‚          â”‚
â”‚   â”‚   â”‚   Cores     â”‚  Shared â”‚   Cores     â”‚                  â”‚          â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Memory â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚          â”‚
â”‚   â”‚         â†‘                       â†‘                          â”‚          â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚          â”‚
â”‚   â”‚              No data copying!                              â”‚          â”‚
â”‚   â”‚                                                             â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â”‚   Benefits:                                                                 â”‚
â”‚   â€¢ No CPUâ†”GPU memory transfers                                            â”‚
â”‚   â€¢ Larger effective VRAM (uses system RAM)                                â”‚
â”‚   â€¢ Lower latency                                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MLX-LM Quick Start

```python
from mlx_lm import load, generate

# Load model
model, tokenizer = load("Qwen/Qwen3-0.6B")

# Generate
response = generate(
    model, 
    tokenizer, 
    prompt="What is machine learning?",
    max_tokens=200
)
print(response)
```

### MLX vs PyTorch on Apple Silicon

| Aspect | PyTorch (MPS) | MLX |
|--------|---------------|-----|
| Memory efficiency | Good | Excellent |
| Native optimization | Partial | Full |
| API style | PyTorch | NumPy-like |
| Training support | Yes | Yes |
| Inference speed | Good | Better |
| Compatibility | Cross-platform | Apple only |

### When to Use MLX

| Scenario | Recommendation |
|----------|----------------|
| Mac inference | MLX |
| Mac fine-tuning | MLX or PyTorch |
| Cross-platform code | PyTorch |
| Production on Linux | PyTorch/vLLM |

---

## Adapter Formats & Conversion

### PEFT vs MLX Adapter Formats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Adapter Formats                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   PEFT/HuggingFace Format:           MLX Format:                            â”‚
â”‚   â”œâ”€â”€ adapter_config.json            â”œâ”€â”€ adapter_config.json                â”‚
â”‚   â”œâ”€â”€ adapter_model.safetensors      â”œâ”€â”€ adapters.safetensors  â† Different!â”‚
â”‚   â””â”€â”€ tokenizer files                â””â”€â”€ tokenizer files                    â”‚
â”‚                                                                             â”‚
â”‚   Trained with: PEFT library         Trained with: mlx_lm.lora             â”‚
â”‚   Works with: HuggingFace, vLLM      Works with: MLX-LM                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Converting PEFT to MLX: Merge First

Since PEFT adapters can't be directly loaded by MLX-LM, you need to merge them first:

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load base model and adapter
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B")
model = PeftModel.from_pretrained(base_model, "./lora-adapter")

# Merge weights
merged_model = model.merge_and_unload()

# Save merged model
merged_model.save_pretrained("./merged-model")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
tokenizer.save_pretrained("./merged-model")
```

Then load with MLX:

```python
from mlx_lm import load, generate

model, tokenizer = load("./merged-model")
```

### Options for Using Adapters

| Method | Format | Hot-swappable | Memory |
|--------|--------|---------------|--------|
| Merge + Load | Merged model | No | Full model |
| PEFT runtime | PEFT adapter | Yes | Base + adapter |
| MLX native adapter | MLX adapter | Yes | Base + adapter |
| vLLM LoRA | PEFT adapter | Yes | Base + adapter |

---

## Building Inference Servers

### Option 1: Native MLX-LM Server

```bash
# Start server
uv run python -m mlx_lm.server --model ./model --port 8000

# Test (OpenAI-compatible)
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### Option 2: Custom FastAPI Server

```python
from fastapi import FastAPI
from pydantic import BaseModel
from mlx_lm import load, generate
import uvicorn

# Load model at startup
model, tokenizer = load("./model")

app = FastAPI(title="LLM Server")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 200

class GenerateResponse(BaseModel):
    prompt: str
    response: str

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/generate")
def generate_text(request: GenerateRequest):
    response = generate(
        model, tokenizer,
        prompt=request.prompt,
        max_tokens=request.max_tokens,
    )
    return GenerateResponse(prompt=request.prompt, response=response)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Option 3: HuggingFace with FastAPI

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from fastapi import FastAPI
import torch

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

app = FastAPI()

@app.post("/generate")
def generate(prompt: str, max_tokens: int = 100):
    result = pipe(prompt, max_new_tokens=max_tokens)
    return {"response": result[0]["generated_text"]}
```

### Server Comparison

| Feature | MLX-LM Server | Custom FastAPI | vLLM Server |
|---------|---------------|----------------|-------------|
| OpenAI API | âœ… | Manual | âœ… |
| Streaming | âœ… | Manual | âœ… |
| Batching | âŒ | Manual | âœ… (continuous) |
| LoRA hot-swap | âŒ | Manual | âœ… |
| Platform | Mac only | Any | CUDA only |

---

## Code Walkthrough

### File: `01_peft_to_mlx.py`

Converting PEFT adapter to merged model:

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load and merge
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B")
model = PeftModel.from_pretrained(base_model, "./adapter")
merged_model = model.merge_and_unload()

# Save
merged_model.save_pretrained("./merged")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
tokenizer.save_pretrained("./merged")
```

### File: `02_mlx_lora.py`

Comparing base vs fine-tuned model:

```python
from mlx_lm import load, generate

# Load both models
base_model, base_tokenizer = load("Qwen/Qwen3-0.6B")
merged_model, merged_tokenizer = load("./merged-model")

# Compare outputs
prompts = [
    "What are the symptoms of diabetes?",
    "How is hypertension treated?",
]

for prompt in prompts:
    print(f"\n=== {prompt} ===")
    
    print("\nğŸ”µ BASE:")
    print(generate(base_model, base_tokenizer, prompt=prompt, max_tokens=200))
    
    print("\nğŸŸ£ FINE-TUNED:")
    print(generate(merged_model, merged_tokenizer, prompt=prompt, max_tokens=200))
```

### File: `03_server.py`

FastAPI inference server:

```python
from fastapi import FastAPI
from pydantic import BaseModel
from mlx_lm import load, generate
import uvicorn

MODEL_PATH = "./merged-model"
model, tokenizer = load(MODEL_PATH)

app = FastAPI(title="MLX-LM Inference Server")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 200

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/generate")
def generate_text(request: GenerateRequest):
    response = generate(model, tokenizer, 
                       prompt=request.prompt, 
                       max_tokens=request.max_tokens)
    return {"prompt": request.prompt, "response": response}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Run:**
```bash
uv run python src/inferencing_and_advanced/03_server.py
```

**Test:**
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is diabetes?", "max_tokens": 100}'
```

---

## Summary

| Concept | Key Takeaway |
|---------|--------------|
| **SafeTensors** | Preferred format, safe & fast |
| **Quantization** | INT4/INT8 for memory savings |
| **KV Cache** | Often larger than model weights |
| **MLX** | Best for Apple Silicon inference |
| **Adapter conversion** | Merge PEFT â†’ load with MLX |
| **Servers** | FastAPI for custom, mlx_lm.server for quick |

---

## Next Steps

- [04_tool_calling.md](04_tool_calling.md) - Add tool/function calling
- [05_deployment.md](05_deployment.md) - Production deployment with vLLM

